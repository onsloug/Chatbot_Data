{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_extraction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjnSuMVtHwwWfMuBqXTtzT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DXISgQuj3XVZ"},"source":["# Downloading Data\r\n","Downloading original dataset from https://github.com/ldulcic/chatbot saved in dropbox"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwRIcJKk3Zi7","executionInfo":{"status":"ok","timestamp":1607624322341,"user_tz":0,"elapsed":45955,"user":{"displayName":"Stefano Rovedi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmRlpFw-YloYq0uWOewsmSRGehizZNbuvovA8U=s64","userId":"07718080372377058442"}},"outputId":"a7b54feb-bb93-4701-f77b-b13a448d1fee"},"source":["!wget https://www.dropbox.com/s/nmnlcncn7jtb7i9/twcs.zip\r\n","!unzip twcs.zip\r\n","!mkdir data\r\n","!mv twcs.csv data\r\n","!rm twcs.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-12-10 18:18:32--  https://www.dropbox.com/s/nmnlcncn7jtb7i9/twcs.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.1, 2620:100:6017:18::a27d:212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/nmnlcncn7jtb7i9/twcs.zip [following]\n","--2020-12-10 18:18:32--  https://www.dropbox.com/s/raw/nmnlcncn7jtb7i9/twcs.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com/cd/0/inline/BE0kAAZY2Gf9ekSLTJFVhCk8MdC78q1X8p4DaW9p3lwuMqcihTUTUDKor1mPdXslobqVQcaZg2y1uH9oYh3PyUSaxmThwwc77iu8_tUirDFAxA/file# [following]\n","--2020-12-10 18:18:33--  https://ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com/cd/0/inline/BE0kAAZY2Gf9ekSLTJFVhCk8MdC78q1X8p4DaW9p3lwuMqcihTUTUDKor1mPdXslobqVQcaZg2y1uH9oYh3PyUSaxmThwwc77iu8_tUirDFAxA/file\n","Resolving ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com (ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n","Connecting to ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com (ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BE3Cca9NUJBBqxlcvF0W_qjaTc1bSTXYJlSQnA-0DrXLbA6o26IQJ_0exCc4MpIuUHYlMl0WmwT5hU_yodj9vf5M41HIAoV_RZWie1kaeuveneqEEziFeJ1PFHAxKmKxRhE0b1Xja3JDk_HTRuo5eTzYT94Mvb8IrmTVWC5v6hB9bZ_mFlXtUSrQzZVevNEMfqYe2NvNhLO8eYGpJCj5L0ZuGx4LEmY_pg-s6fRQHQjapCnMbpLg-ev0K8h-KKTMlL6rwJEb_eSruG-LKFpNlLjk8cYCQH1C-ls_hJE9FJLM8rrE-cRsk0CXdQpAaH8CM9T6mC-1wGi3c3-H96fXLrf0/file [following]\n","--2020-12-10 18:18:33--  https://ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com/cd/0/inline2/BE3Cca9NUJBBqxlcvF0W_qjaTc1bSTXYJlSQnA-0DrXLbA6o26IQJ_0exCc4MpIuUHYlMl0WmwT5hU_yodj9vf5M41HIAoV_RZWie1kaeuveneqEEziFeJ1PFHAxKmKxRhE0b1Xja3JDk_HTRuo5eTzYT94Mvb8IrmTVWC5v6hB9bZ_mFlXtUSrQzZVevNEMfqYe2NvNhLO8eYGpJCj5L0ZuGx4LEmY_pg-s6fRQHQjapCnMbpLg-ev0K8h-KKTMlL6rwJEb_eSruG-LKFpNlLjk8cYCQH1C-ls_hJE9FJLM8rrE-cRsk0CXdQpAaH8CM9T6mC-1wGi3c3-H96fXLrf0/file\n","Reusing existing connection to ucad9b6a28b080b7d32993d8a3e6.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 175021289 (167M) [application/zip]\n","Saving to: ‘twcs.zip’\n","\n","twcs.zip            100%[===================>] 166.91M  98.5MB/s    in 1.7s    \n","\n","2020-12-10 18:18:35 (98.5 MB/s) - ‘twcs.zip’ saved [175021289/175021289]\n","\n","Archive:  twcs.zip\n","  inflating: twcs.csv                \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4uYEyAId3nxN"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aE130SGt3SSb","executionInfo":{"status":"ok","timestamp":1607624313011,"user_tz":0,"elapsed":36994,"user":{"displayName":"Stefano Rovedi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmRlpFw-YloYq0uWOewsmSRGehizZNbuvovA8U=s64","userId":"07718080372377058442"}},"outputId":"ef5c1cf6-446c-4399-add5-e960be5c3b39"},"source":["!pip install emoji\r\n","!pip install spacy_cld\r\n","!pip install preprocessor\r\n","!pip install tweet-preprocessor"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n","\r\u001b[K     |██████▍                         | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49715 sha256=0cc70f30b69b4ba83c971d7ea52f42fd04a0a93e3effaec15905c72b7fd8f46c\n","  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n","Collecting spacy_cld\n","  Downloading https://files.pythonhosted.org/packages/e3/3b/f5344007259b5beb0a8e0d7b9e6b0d2c5c4dcfe674bc94b7497bcc201ee0/spacy_cld-0.1.0.tar.gz\n","Requirement already satisfied: spacy<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy_cld) (2.2.4)\n","Collecting pycld2>=0.31\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/d2/8b0def84a53c88d0eb27c67b05269fbd16ad68df8c78849e7b5d65e6aec3/pycld2-0.41.tar.gz (41.4MB)\n","\u001b[K     |████████████████████████████████| 41.4MB 110kB/s \n","\u001b[?25hRequirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.18.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (50.3.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (7.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (0.8.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.0.0->spacy_cld) (2.0.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.0.0->spacy_cld) (2.10)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.0.0->spacy_cld) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.0.0->spacy_cld) (3.4.0)\n","Building wheels for collected packages: spacy-cld, pycld2\n","  Building wheel for spacy-cld (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spacy-cld: filename=spacy_cld-0.1.0-cp36-none-any.whl size=4065 sha256=5c57ad4e687004d45dbc3d49f8a90f4b8c1b0895496a7fa319017c93e4ca683f\n","  Stored in directory: /root/.cache/pip/wheels/7e/a6/a5/604befa6807cc78a6852be9e933c080362b2498fca796cd34e\n","  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycld2: filename=pycld2-0.41-cp36-cp36m-linux_x86_64.whl size=9833553 sha256=fd2aa66947a5b38261587dee77e4cd03a72c358899ff3db2db4e5f0acce1ad27\n","  Stored in directory: /root/.cache/pip/wheels/c6/8f/e9/08a1a8932a490175bd140206cd86a3dbcfc70498100de11079\n","Successfully built spacy-cld pycld2\n","Installing collected packages: pycld2, spacy-cld\n","Successfully installed pycld2-0.41 spacy-cld-0.1.0\n","Collecting preprocessor\n","  Downloading https://files.pythonhosted.org/packages/96/ad/d9f4ffb9bb97d1cb5bcb876b7932571d4dbaa3eff1701ad45d367f0ea27b/preprocessor-1.1.3.tar.gz\n","Building wheels for collected packages: preprocessor\n","  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for preprocessor: filename=preprocessor-1.1.3-cp36-none-any.whl size=4478 sha256=f4db94d83ca84b6ab85537a3cd4d60275bf45aea2370353ff0e8ee575b0fddb3\n","  Stored in directory: /root/.cache/pip/wheels/98/c1/a2/21fbcfd80d76576bbf148991a66f00730f541f265c7600000f\n","Successfully built preprocessor\n","Installing collected packages: preprocessor\n","Successfully installed preprocessor-1.1.3\n","Collecting tweet-preprocessor\n","  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qs_MUovF4AmX","executionInfo":{"status":"ok","timestamp":1607624324284,"user_tz":0,"elapsed":1933,"user":{"displayName":"Stefano Rovedi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmRlpFw-YloYq0uWOewsmSRGehizZNbuvovA8U=s64","userId":"07718080372377058442"}}},"source":["import pandas as pd\r\n","import os\r\n","import re\r\n","import spacy\r\n","from sklearn.model_selection import train_test_split\r\n","import emoji\r\n","from spacy_cld import LanguageDetector\r\n","import preprocessor as p"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUHIrpKY4FEl","executionInfo":{"status":"ok","timestamp":1607624324286,"user_tz":0,"elapsed":1931,"user":{"displayName":"Stefano Rovedi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmRlpFw-YloYq0uWOewsmSRGehizZNbuvovA8U=s64","userId":"07718080372377058442"}}},"source":["DATA_FOLDER = os.path.expanduser('data/')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"6UZ5Lk6e4KeH"},"source":["def id2text(df, x):\r\n","    row = df[df.tweet_id == int(x)]\r\n","    # some ids are missing, they just don't exist in data\r\n","    return '' if (len(row) == 0) else row.iloc[0]['text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q97MWrqU4La_"},"source":["def clean_tweet(tweet):\r\n","    # removes @ mentions, hashtags, emojis, twitter reserved words and numbers\r\n","    p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER)\r\n","    clean = p.clean(tweet)\r\n","\r\n","    # transforms every url to \"<url>\" token and every hashtag to \"<hashtag>\" token\r\n","    p.set_options(p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG, p.OPT.URL)\r\n","    clean = p.tokenize(clean)\r\n","    clean = re.sub(r'\\$HASHTAG\\$', '<hashtag>', clean)\r\n","    clean = re.sub(r'\\$URL\\$', '<url>', clean)\r\n","\r\n","    # preprocessor doesn't seem to clean all emojis so we run text trough emoji regex to clean leftovers\r\n","    clean = re.sub(emoji.get_emoji_regexp(), '', clean)\r\n","\r\n","    # removing zero-width character which is often bundled with emojis\r\n","    clean = re.sub(u'\\ufe0f', '', clean)\r\n","\r\n","    # remove multiple empty spaces with one\r\n","    clean = re.sub(r' +', ' ', clean)\r\n","\r\n","    # replace &gt; and &lt;\r\n","    clean = re.sub(r'&gt;', '>', clean)\r\n","    clean = re.sub(r'&lt;', '<', clean)\r\n","\r\n","    # strip any leftover spaces at the beginning and end\r\n","    clean = clean.strip()\r\n","\r\n","    return clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zOHLq_j84QTE"},"source":["def set_empty_if_not_english(nlp, x):\r\n","    doc = nlp(x)\r\n","    # TODO tweets with no language tend to be valid, but they also tend to be gibberish, filtering for now\r\n","    # TODO don't filter scotish, don't filter danish with \"icloud\" in it (for some reason every tweet with icloud in it is classified as danish), filtering for now\r\n","    return x if doc._.languages and ('en' in doc._.language_scores and doc._.language_scores['en'] >= 0.80) else ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrQE2Iwj4TtB"},"source":["def qa_from_author(df, nlp, author_id):\r\n","    \"\"\"\r\n","    Creates qa dataset (in form of dataframe) from all tweets of author (identified by author_id)\r\n","    :param df: All twitter customer support data as dataframe.\r\n","    :param author_id: Name of author.\r\n","    :return: Dataframe containing 'question' and 'answer' fields where 'question' is user tweet and 'answer' is customer\r\n","            support tweet\r\n","    \"\"\"\r\n","    # get all tweets from certain support service\r\n","    support_service = df[df.author_id == author_id]\r\n","    # remove tweets which are not triggered by user tweet (there is no Q(uestion))\r\n","    support_service = support_service[~support_service.in_response_to_tweet_id.isnull()]\r\n","\r\n","    # take column we are interested in\r\n","    support_service = support_service[['author_id', 'text', 'in_response_to_tweet_id']]\r\n","\r\n","    # replace tweet ids with actual tweet text\r\n","    support_service.loc[:, 'in_response_to_tweet_id'] = support_service.in_response_to_tweet_id.apply(lambda x: id2text(df, x))\r\n","\r\n","    # rename and rearrange columns\r\n","    support_service.rename(columns={'author_id': 'author_id', 'text': 'answer', 'in_response_to_tweet_id': 'question'},\r\n","                           inplace=True)\r\n","    support_service = support_service[['author_id', 'question', 'answer']]\r\n","\r\n","    # clean twitter data\r\n","    support_service.loc[:, 'question'] = support_service.question.apply(clean_tweet)\r\n","    support_service.loc[:, 'answer'] = support_service.answer.apply(clean_tweet)\r\n","\r\n","    # filter all languages which are not english (non-english tweets will be set to empty string and then filtered at\r\n","    # the end of this method)\r\n","    support_service.loc[:, 'question'] = support_service.question.apply(lambda x: set_empty_if_not_english(nlp, x))\r\n","    support_service.loc[:, 'answer'] = support_service.answer.apply(lambda x: set_empty_if_not_english(nlp, x))\r\n","\r\n","    # remove all QA pairs where Q or A are empty or contain only dot (.)\r\n","    support_service = support_service[~(support_service.question == '') & ~(support_service.answer == '')]\r\n","    support_service = support_service[~(support_service.question == '.') & ~(support_service.answer == '.')]\r\n","\r\n","    return support_service"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpviJEZq4XVx"},"source":["def split_dataset(path, random_state=287):\r\n","    dir_name = os.path.dirname(path)\r\n","    file_name = os.path.basename(path).split('.')[0]  # file name must end in '.tsv'\r\n","\r\n","    df = pd.read_csv(path, sep='\\t')\r\n","\r\n","    train, rest = train_test_split(df, test_size=0.2, random_state=random_state)\r\n","    val, test = train_test_split(rest, test_size=0.5, random_state=random_state)\r\n","\r\n","    # write train, val, test\r\n","    train.to_csv(dir_name + os.path.sep + file_name + '-train.tsv', sep='\\t', index=False)\r\n","    val.to_csv(dir_name + os.path.sep + file_name + '-val.tsv', sep='\\t', index=False)\r\n","    test.to_csv(dir_name + os.path.sep + file_name + '-test.tsv', sep='\\t', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MdJyU1ri4aKg"},"source":["def create_dataset(df, author_ids, nlp):\r\n","    dataset = qa_from_author(df, nlp, author_ids[0])\r\n","    for author_id in author_ids[1:]:\r\n","        dataset = pd.concat([dataset, qa_from_author(df, nlp, author_id)])\r\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wMmMSDj4cgG"},"source":["def create_and_write_dataset(df, nlp, author_id, path):\r\n","    \"\"\"\r\n","    Creates tsv dataset which contains only Apple support conversations with customers.\r\n","    \"\"\"\r\n","    dataset = create_dataset(df, [author_id], nlp)\r\n","    dataset_path = path + author_id.lower() + '.tsv'\r\n","    dataset.to_csv(dataset_path, sep='\\t', index=False)\r\n","    split_dataset(dataset_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8La_Z_T4eoZ"},"source":["def create_all_dataset(df, nlp, path):\r\n","    \"\"\"\r\n","    Creates tsv dataset which contains many customer support services from dataset. Included support service authors are\r\n","    'AppleSupport', 'AmazonHelp', 'Uber_Support', 'Delta', 'SpotifyCares', 'Tesco', 'AmericanAir',\r\n","                  'comcastcares', 'TMobileHelp', 'British_Airways', 'SouthwestAir', 'Ask_Spectrum' and  'hulu_support'\r\n","    \"\"\"\r\n","    author_ids = ['AppleSupport', 'AmazonHelp', 'Uber_Support', 'Delta', 'SpotifyCares', 'Tesco', 'AmericanAir',\r\n","                  'comcastcares', 'TMobileHelp', 'British_Airways', 'SouthwestAir', 'Ask_Spectrum', 'hulu_support']\r\n","    dataset = create_dataset(df, author_ids, nlp)\r\n","    dataset = dataset.sample(frac=1)  # shuffle dataset\r\n","    dataset_path = path + 'twitter-all' + '.tsv'\r\n","    dataset.to_csv(dataset_path, sep='\\t', index=False)\r\n","    split_dataset(dataset_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFo98qTW4kuE"},"source":["def main():\r\n","    df = pd.read_csv(DATA_FOLDER + 'twcs.csv')\r\n","    df.sort_values(by='tweet_id', inplace=True)\r\n","\r\n","    nlp = spacy.load('en')\r\n","    nlp.add_pipe(LanguageDetector())\r\n","\r\n","    create_and_write_dataset(df, nlp, 'AppleSupport', DATA_FOLDER)\r\n","    create_and_write_dataset(df, nlp, 'AmazonHelp', DATA_FOLDER)\r\n","    create_and_write_dataset(df, nlp, 'Uber_Support', DATA_FOLDER)\r\n","    create_and_write_dataset(df, nlp, 'Delta', DATA_FOLDER)\r\n","    create_and_write_dataset(df, nlp, 'SpotifyCares', DATA_FOLDER)\r\n","\r\n","\r\n","if __name__ == '__main__':\r\n","    main()"],"execution_count":null,"outputs":[]}]}